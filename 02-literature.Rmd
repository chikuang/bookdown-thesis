# Optimal Regression Designs Under SLSE {#chapter-SLSE}

In this chapter, we first review the results and properties of optimal regression designs under the SLSE. We then derive several new analytical results for the optimal designs under the SLSE. We begin with recalling the formulation of the optimal regression designs under SLSE based on three different criteria, A-, D-, and c-optimality. The formulation of A-, and D-optimality under the SLSE was first proposed in @gao2014new while A-optimality was further investigated in @yin2018optimal. We formulate optimal design problems under A-optimality differently so that the properties can be extended to c-optimality which has not been studied yet. Equivalence results for verifying optimal designs are also obtained. In addition, analytical results are derived for the number of support points for several regression models.

## Design criteria under SLSE

Let us introduce the notations first. Assume $\sigma_o$ and $\boldsymbol{\theta}_o$ are the true parameter values of $\sigma$ and $\boldsymbol{\theta}$, respectively. Let $S \subset \mathbb{R}^p$ be the design space for $\boldsymbol{x}$. Let $\operatorname{tr}(\cdot)$ and $\det(\cdot)$ be the trace and determinant functions of a matrix, respectively. Moreover, let $\Xi$ denote the class of all probability measures on $S$. Define, for any $\xi\in \Xi$,

\begin{equation*}
\boldsymbol{g}_1=\boldsymbol{g}_1(\xi,\boldsymbol{\theta_o})=\mathbb{E}_{\xi}\bigg[ \frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_o} \bigg],
\end{equation*}

and 
\begin{equation*}
\boldsymbol{G}_2=\boldsymbol{G}_2(\xi,\boldsymbol{\theta}_o)=\mathbb{E}_{\xi} \bigg[ \frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}}\frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{\partial \boldsymbol{\theta}^\top}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta}_o}\bigg].
\end{equation*}


For a discrete probability measure $\xi\in \Xi$, we write it as
\[
\xi=\begin{bmatrix}
	\boldsymbol{x}_1&\boldsymbol{x}_2	&\ldots	&\boldsymbol{x}_m\\
    p_1		&p_2		&\ldots	&p_m
\end{bmatrix},
\]
where $\boldsymbol{x}_1,\boldsymbol{x}_2,\dots,\boldsymbol{x}_m$ are the support points in $S$, and $p_1,\dots,p_m$ are the probabilities associated with those points. From @gao2014new, the asymptotic variance-covariance matrix of $\hat{\boldsymbol{\theta}}$ under the SLSE is given by
\begin{equation}
\boldsymbol{\mathbb{V}}({\boldsymbol{\hat{\theta}}})=\sigma^2_o(1-t)(\boldsymbol{G}_2-t\boldsymbol{g}_1\boldsymbol{g}_1^\top)^{-1},
(\#eq:cov-matrix)
\end{equation}
where $t=\frac{\mu_3^2}{\sigma_o^2(\mu_4-\sigma_o^4)}$, $\mu_3=\mathbb{E}[\epsilon_i^3|\boldsymbol{x}]$ and $\mu_4=\mathbb{E}[\epsilon_i^4|\boldsymbol{x}]$. Note that @gao2014new discussed that $t \in [0,1)$ for any error distributions, and $t=0$ for symmetric error distributions. Define a matrix $\boldsymbol{J}=\boldsymbol{J}(\xi,\boldsymbol{\theta}_o,t) = \boldsymbol{G}_2-t\boldsymbol{g}_1\boldsymbol{g}_1^T$. It is clear that matrix $\boldsymbol{J}$ is proportional to the inverse of the variance-covariance matrix \@ref(eq:cov-matrix). For the rest of the thesis, we will be working on the design problems using matrix $\boldsymbol{J}$. 


As discussed in Chapter \@ref(intro), we aim to minimize the loss functions in optimal design problems, and the loss functions for D-, A- and c-optimality criteria under SLSE can be expressed as
\begin{equation}
\begin{aligned}
  \phi_D(\xi,\boldsymbol{\theta_o},t)&=&\det(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
  \phi_A(\xi,\boldsymbol{\theta_o},t)&=&\operatorname{tr}(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
  \phi_c(\xi,\boldsymbol{\theta_o},t)&=&\boldsymbol{c}_1^\top\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{c}_1,
\end{aligned}
(\#eq:loss-J)
\end{equation}
when $\boldsymbol{J}$ is non-singular, and $\boldsymbol{c}_1$ is a given vector in $\mathbb{R}^q$. If $\boldsymbol{J}$ is singular, all the three loss functions are defined to be $+\infty$. We use $\xi_D^*,~\xi_A^*$ and $\xi_c^*$ to denote for \text{D-,} A- and c-optimal designs, respectively. For two measures $\xi_1$ and $\xi_2\in \Xi$, define $\xi_{\alpha}=(1-\alpha)\xi_1+\alpha \xi_2$ for $\alpha \in[0,1]$.

::: {.lemma}
$\log(\phi_D(\xi_{\alpha},\boldsymbol{\theta_o},t))$, $\phi_A(\xi_{\alpha},\boldsymbol{\theta_o},t)$ and $\phi_c(\xi_{\alpha},\boldsymbol{\theta_o},t)$ are convex functions of $\alpha$.
:::
The convexity results are discussed in @boyd2004convex and @wong2019cvx. Similar convexity results are given in @bose2015optimal. We will use $\log(\det(\boldsymbol{J}^{-1}(\xi,\boldsymbol{\theta_o},t)))$ for D-optimal design for the rest of the thesis as $\log(\cdot)$ is a monotonic increasing function which does not change the optimality.


Although we have formulated the loss functions, there are some issues associated with the formulation in \@ref{eq-loss-J}. The reason is that $\boldsymbol{J}$ is lacking of linearity. From construction of $\boldsymbol{J}$, $\boldsymbol{J}(\xi_{\alpha},\boldsymbol{\theta_o},t)$ is not a linear combination of $\boldsymbol{J}(\xi_{1},\boldsymbol{\theta_o},t)$ and $\boldsymbol{J}(\xi_{2},\boldsymbol{\theta_o},t)$. Thus, it is difficult to obtain the theoretical results using $\boldsymbol{J}$. To solve this issue, @gao2017d proposed an alternative expression for characterizing the loss functions. The key is to define a matrix
\begin{equation}
\boldsymbol{B}(\xi)=\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1				&	\sqrt{t}\boldsymbol{g_1}^\top\\
\sqrt{t}\boldsymbol{g}_1	&	\boldsymbol{G}_2
\end{pmatrix},
 (\#eq:B-matrix)
\end{equation}
which plays an important role in the following formulation. Note $\boldsymbol{B}(\xi_{\alpha})$ is now an affine function of $\alpha$, i.e., 
$$ \boldsymbol{B}(\xi_{\alpha})=(1-\alpha)\boldsymbol{B}(\xi_1)+\alpha\boldsymbol{B}(\xi_2).$$
This fact ultimately makes $\boldsymbol{B}$ much more useful than $\boldsymbol{J}$ to study optimal designs under SLSE. The inverse of $\boldsymbol{B}$ is given as
\begin{equation}
\boldsymbol{B}^{-1}(\xi)=\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)	=\begin{pmatrix}
\frac{1}{r}				&	\frac{-\sqrt{t}}{r}\boldsymbol{g}_1^\top\boldsymbol{G}_2^{-1}	\\
\frac{-\sqrt{t}}{r}\boldsymbol{G}_2^{-1}\boldsymbol{g}_1	&\boldsymbol{J}^{-1}
\end{pmatrix},
 (\#eq:B-inverse)
\end{equation}
where $r=1-t\boldsymbol{g}_1^\top\boldsymbol{G}_2^{-1}\boldsymbol{g}_1$. Note that if $\boldsymbol{J}$ is invertible, $\boldsymbol{G}_2$ must also be invertible since $\boldsymbol{G}_2=\boldsymbol{J}+t\boldsymbol{g}_1\boldsymbol{g}_1^\top$ and $t\boldsymbol{g}_1\boldsymbol{g}_1^\top$ is positive semi-definite. Consequently, $\boldsymbol{B}^{-1}$ exists from \@ref(eq:B-inverse). Now, we are going to present the following lemmas to characterize the loss functions for A-, c- and D-optimal design problems. Lemma \@ref(lem:loss-A) is slightly different from a result in @yin2018optimal, Lemma \@ref(lem:loss-D) is a result from @gao2017d, and Lemma \@ref(lem:loss-c) is a new result.

::: {.lemma #loss-A}
If $\boldsymbol{J}$ is invertible, then
  \begin{equation*}
  \phi_A(\xi,\boldsymbol{\theta}_o,t)=\operatorname{tr}(\boldsymbol{J}^{-1})=\operatorname{tr}(\boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C}),
  \end{equation*}
where $\boldsymbol{C}=0\oplus \boldsymbol{I}_q$, $\boldsymbol{I}_q$ denotes for the $q\times q$ identity matrix, and $\oplus$ denotes for matrix direct sum operator.
:::

::: {.proof}
From \eqref{B inverse} and $\boldsymbol{C}=0\oplus \boldsymbol{I_q}$, we get
	\begin{align*}
    \boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C}&=	\begin{pmatrix}
    							0	&	0\\
                                0	&	\boldsymbol{I}_q
    							\end{pmatrix}^T\begin{pmatrix}
            \frac{1}{r}				&	\frac{-\sqrt{t}}{r}\boldsymbol{g_1}^\top\boldsymbol{G}_2^{-1}	\\
            \frac{-\sqrt{t}}{r}\boldsymbol{g}_1\boldsymbol{G}_2^{-1}	&\boldsymbol{J}^{-1}
            \end{pmatrix}\begin{pmatrix}
    							0	&	0\\
                                0	&	\boldsymbol{I}_q
    							\end{pmatrix}\\
            				&= \begin{pmatrix}
    							0	&	0\\
                                0	&	\boldsymbol{J}^{-1}
    							\end{pmatrix},
    \end{align*}
    which implies $\operatorname{tr}(\boldsymbol{C}^\top\boldsymbol{B}^{-1}\boldsymbol{C})=\operatorname{tr}(\boldsymbol{J}^{-1})$.
:::

::: {.lemma #loss-D}
If $\boldsymbol{J}$ is invertible, then
\begin{equation*}
	\phi_D(\xi,\boldsymbol{\theta_o},t)=\det(\boldsymbol{J}^{-1})=\det(\boldsymbol{B}^{-1}).
\end{equation*}
:::


::: {..proof}
From \@ref{eq:B-matrix}, we have
	\begin{align*}
    \det(\boldsymbol{B})	&= \det(1-t\boldsymbol{g_1}^T\boldsymbol{G_2}^{-1}\boldsymbol{g_1})\det(\boldsymbol{G_2})\\
    		&= \det(\boldsymbol{I}-t\boldsymbol{g_1}\boldsymbol{g_1}^\top\boldsymbol{G_2}^{-1}) \det(\boldsymbol{G_2}) \\
    		&=	\det(\boldsymbol{G_2}-t\boldsymbol{g_1}\boldsymbol{g_1}^T)\\
    		&= \det(\boldsymbol{J}),
    \end{align*}
    which gives $\det(\boldsymbol{J}^{-1})=\det(\boldsymbol{B}^{-1})$.
:::

:::{.lemma #loss-c}
If $\boldsymbol{J}$ is invertible, then
\begin{equation*}
	\phi_c(\xi,\boldsymbol{\theta_o},t)=\boldsymbol{c}_1\top\boldsymbol{J}^{-1}\boldsymbol{c}_1=\boldsymbol{c}^\top\boldsymbol{B}^{-1}\boldsymbol{c},
\end{equation*}
where $\boldsymbol{c_1}$ is a vector in $\mathbb{R}^q$ and $\boldsymbol{c}^\top=(0,\boldsymbol{c}_1^\top)$.
:::

Thus, by Lemmas \ref{lemma:loss A}, \ref{lemma:loss D} and \ref{lemma:loss c}, the alternative expressions for the loss functions in \eqref{loss J} are 
\begin{equation} \label{loss B}
\begin{aligned}
	\phi_D(\xi,\boldsymbol{\theta_o},t))&=&\det(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)),\\
	\phi_A(\xi,\boldsymbol{\theta}_o,t)&=&\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{C}),\\
    \phi_c(\xi,\boldsymbol{\theta_o},t)&=&\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o},t)\boldsymbol{c},
\end{aligned}
\end{equation}
where $\boldsymbol{C}= 0 \oplus \boldsymbol{I_q}$, $\boldsymbol{c_1}\in \mathbb{R}^q$ and $\boldsymbol{c}^T=(0,\boldsymbol{c_1}^T)$. If $\boldsymbol{B}$ is singular, all the three loss functions are defined to be $+\infty$.

## Equivalence theorem for optimal designs under SLSE

In this section we derive the optimality conditions for the optimal designs under the SLSE which follows from the equivalence theorem in @kiefer-wolfowitz1959optimum and @kiefer1974general. We also analyze the minimum number of support points in optimal designs for various regression models, and theoretical results are obtained. Note we study approximate designs in this thesis. The advantages of working with approximate designs instead of exact design are well documented in @kiefer1985jack.

Define a vector $f(\boldsymbol{x,\theta_o})=\frac{\partial g(\boldsymbol{x};\boldsymbol{\theta})}{ \partial \boldsymbol{\theta}}\Big|_{\boldsymbol{\theta}=\boldsymbol{\theta_o}} \in \mathbb{R}^q$ and a matrix 
\begin{equation}
\boldsymbol{M}(\boldsymbol{x})=\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)=\begin{pmatrix}
1		&	\sqrt{t}f^T(\boldsymbol{x},\boldsymbol{\theta_o})\\
\sqrt{t}f(\boldsymbol{x},\boldsymbol{\theta_o})	&f(\boldsymbol{x},\boldsymbol{\theta_o})f^T(\boldsymbol{x},\boldsymbol{\theta_o})
\end{pmatrix}_{(q+1)\times (q+1)}.
 (\#eq:M-matrix)
\end{equation}




Then $\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)=\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)]$. Define dispersion functions
\begin{equation} \label{fuction:dispersion}
\begin{aligned}
  &d_D(\boldsymbol{x},\xi,t) = \operatorname{tr}(\boldsymbol{B}^{-1}\boldsymbol{M}(\boldsymbol{x}))-(q+1),\\
  &d_A(\boldsymbol{x},\xi,t) = \operatorname{tr}(\boldsymbol{M}(\boldsymbol{x})\boldsymbol{B}^{-1}\boldsymbol{C}^T\boldsymbol{C}\boldsymbol{B}^{-1})-\operatorname{tr}(\boldsymbol{C}\boldsymbol{B}^{-1}\boldsymbol{C}^T),\\
  &d_c(\boldsymbol{x},\xi,t) = \boldsymbol{c}^T\boldsymbol{B}^{-1}\boldsymbol{M}(\boldsymbol{x})\boldsymbol{B}^{-1}\boldsymbol{c}-\boldsymbol{c}^T\boldsymbol{B}^{-1}\boldsymbol{c},\\
\end{aligned}
\end{equation}
where $\boldsymbol{B}=\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)$ is invertible.


::: {.theorem #dispersion}
We suppose all the dispersion functions are evaluated at $\boldsymbol{\theta_o}$. If $\xi_D^*$, $\xi_A^*$ and $\xi_c^*$ are the optimal probability measures for D-, A- and c- optimality, respectively, then $\boldsymbol{B}$ is invertible and for any $\boldsymbol{x}\in S$,
\begin{align}
d_D(\boldsymbol{x},\xi_D^*,t) &\leq 0, (\#eq:dire-D) \\
d_A(\boldsymbol{x},\xi_A^*,t) &\leq 0, (\#eq:dire-A) \\
d_c(\boldsymbol{x},\xi_c^*,t) &\leq 0. (\#eq:dire-c)
\end{align}
:::

::: {.proof}
In this proof, we use $\boldsymbol{B}(\xi)$ for $\boldsymbol{B}(\xi,\boldsymbol{\theta_o},t)$ and $\boldsymbol{M}(\boldsymbol{x})$ for $\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta_o},t)$. Suppose $\xi^*$ is an optimal design to a criterion. Define $\xi_\alpha=(1-\alpha)\xi^*+\alpha \xi$ where $\xi$ is an arbitrary probability measure. This proof is based on Kiefer's general equivalence theorem \citep{kiefer1974general}, and the optimal condition can be derived from  $\frac{\partial \phi(\xi_\alpha)}{\partial \alpha}\big|_{\alpha=0}\geq 0$ for any measure $\xi \in \Xi$, where $\phi$ is a loss function.

We first prove \@ref(eq:dire-D). Let $\xi_D^*$ be the optimal measure under D-optimality. We have
    \begin{align*}
	\frac{\partial \log(\phi_D(\xi_\alpha))}{\partial \alpha}\Big|_{\alpha=0}	
    &=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)(-\boldsymbol{B}(\xi_D^*)+\boldsymbol{B}(\xi)))\\
    &=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\boldsymbol{B}(\xi))+\operatorname{tr}(\boldsymbol{I_{q+1}})\\
    &=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\boldsymbol{B}(\xi))+(q+1))\\
	&=-\operatorname{tr}(\boldsymbol{B}^{-1}(\xi_D^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]-(q+1))\\
    &=-\mathbb{E}_{\xi}[d_D(\boldsymbol{x},\xi_D^*,t)]\\
	&\geq 0,~\text{for any } \xi \text{ on } S,
	\end{align*}
	which implies $d_D(\boldsymbol{x},\xi_D^*,t)\leq 0$, for all $\boldsymbol{x} \in S$.
	
To prove \@ref(eq:dire-A), let $\xi_A^*$ be the optimal measure under A-optimality. We have
    \begin{align*}
	\frac{\partial \phi_A(\xi_\alpha)}{\partial \alpha}\Big|_{\alpha=0}	
    &=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)[\boldsymbol{B}(\xi)-\boldsymbol{B}(\xi_A^*)]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})\\
    &=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{B}(\xi)\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})+\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C})\\
	&=-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}) +\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi^*)\boldsymbol{C})\\
    &= - (\operatorname{tr}(\mathbb{E}_{\xi}[\boldsymbol{M}_{\xi}(\boldsymbol{x},\boldsymbol{\theta_o})]\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi^*))-\operatorname{tr}(\boldsymbol{C}^T\boldsymbol{B}^{-1}(\xi_A^*)\boldsymbol{C}) )\\
    &= -\mathbb{E}_{\xi}[d_A(\boldsymbol{x},\xi_A^*,t)]\\
	&\geq 0,~ \text{for any } \xi \text{ on } S,
	\end{align*}
    which implies $d_A(\boldsymbol{x},\xi_A^*,t)\leq 0$, for all $\boldsymbol{x} \in S$.
    
       
Lastly, to prove \@ref(eq:dire-D), let $\xi_c^*$ be the optimal measure under c-optimality. We have
    \begin{align*}
	\frac{\partial \phi_c(\xi_\alpha)}{\partial \alpha}\Big|_{\alpha=0}	
    &=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)[-\boldsymbol{B}(\xi)+\boldsymbol{B}(\xi_c^*)]\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}\\
    &=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{B}(\xi)\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{B}(\xi_c^*)\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}\\
    &=-\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\mathbb{E}_{\xi}[\boldsymbol{M}(\boldsymbol{x})]\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}(\xi_c^*)^{-1}\boldsymbol{c}\\
	&=-\mathbb{E}_{\xi}[\boldsymbol{c}^T\boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{M}(\boldsymbol{x}) \boldsymbol{B}^{-1}(\xi_c^*)\boldsymbol{c}+\boldsymbol{c}^T\boldsymbol{B}(\xi_c^*)^{-1}\boldsymbol{c}]\\
    &=-\mathbb{E}_{\xi}[d_c(\boldsymbol{x},\xi_c^*,t)]\\
	&\geq 0,~ \text{for any } \xi \text{ on } S,
	\end{align*}
	which implies $d_c(\boldsymbol{x},\xi_c^*,t)\leq 0$, for all $\boldsymbol{x} \in S$.
:::


## Results on the number of support points {#section-support-pt}

Using the results in Theorem \@ref(thm:dispersion) , we can explore the properties of the optimal designs. In @yin2018optimal and @gao2017d, there are some discussions about the number of support points based on computational results. However, there is little discussion on the number of support points theoretically in @gao2017d, and there is still a large gap to be filled in. Hence we derive several results about the number of support points for various models, including polynomial models, fractional polynomial models, Michaelis-Menton model, Peleg model and trigonometric models.


A polynomial regression model of degree $q$ ($q \ge 1$) without intercept is given by
\begin{equation}
y_i=\theta_1 x_i+\theta_2 x_i^2+\cdots+\theta_q x_i^q+\epsilon_i,~x_i\in S=[-1,+1],~i=1,2,\cdots,n.
(\#eq:model-poly)
\end{equation} 
Polynomial regression models are widely used when the response and regressors have curvilinear relationship. Complex nonlinear relationships can be well approximated by polynomials over a small range of the explanatory variables \citep[p.~223]{montgomery2012introduction}. There are different kinds of polynomial models such as orthogonal polynomial models, multi-variable polynomial models, and one variable polynomial models. Polynomial models are often used in design of experiment for the response surface methodology, and there are many applications in industry. For example, see @box1987empirical, @box1978statistics and @khuri1996response. 


A- and D-optimal designs for \@ref(eq:model-poly) under SLSE are symmetric on $S$ [@yin2018optimal,@gao2017d}. In \@ref(eq:model-poly), we have 
$\boldsymbol{f}(x,\boldsymbol{\theta})= \left(x,x^2,\cdots, x^q \right)^\top$ and
\begin{equation}
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=\begin{pmatrix}
1			&\sqrt{t}x	&\sqrt{t}x^2	&...&\sqrt{t}x^q\\
\sqrt{t}x	&	x^2		&x^3	&\cdots	&x^{q+1}\\
\vdots		&\vdots		&\vdots	&\vdots&	\vdots\\
\sqrt{t}x^q	&	x^{q+1}	&\dots	&\dots	&x^{2q}
\end{pmatrix}_{(q+1)\times(q+1)}.
(\#eq:poly)
\end{equation}

::: {.theorem #support}
Let $n_A$ and $n_D$ denote the minimum number of support points in A- and D-optimal designs under SLSE, respectively. For \@ref(eq:model-poly), we have
\begin{equation} \label{support}
n_A ~ = ~ \text{q or q+1},
\end{equation}
and 
\begin{equation} \label{support2}
n_D ~ = ~ \text{q or q+1}.
\end{equation}
:::

::: {.proof}
The proof includes the following three parts.

(i). From \@ref(thm:dispersion} and \@ref(eq:poly), we can see that $d_A(x,\xi_A^*,t)$ and $d_D(x,\xi_D^*,t)$ are polynomial functions of $x$ with highest degree $2q$. By fundamental theorem of algebra, there are exactly $2q$ roots for $x$ in equations $d_A(x,\xi_A^*,t)=0$ and $d_D(x,\xi_D^*,t)=0$. However, we have at most $2q$ real roots.

(ii). By the construction, the determinant of $\boldsymbol{B}$ matrix is not zero if and only if the determinant of $\boldsymbol{G_2}$ is not zero. Therefore, there are at least q support points in $\xi$.

(iii). Both boundary points are the support points, so the number of support points are at most $2q-2$ in the interval $(-1, +1)$. From the equivalence theorem, we know that the dispersion functions are all less or equal to zero (i.e. $d_A(x,\xi_A^*,t)\leq 0$ and $d_D(x,\xi_D^*,t)\leq 0$), so all those support points in $(-1,+1)$ have a multiplicity of two. In total, we have at most $2+\frac{(2q-2)}{2}=q+1$ distinct support points. 

Thus, the number of support points in $\xi_A^*$ and $\xi_D^*$ is either $q$ or $q+1$.
\end{proof}
:::

::: {.example #poly2}
Consider \@ref(eq:model-poly) with $q=2$ and $S=[-1,+1]$. Let $\eta_m=\mathbb{E}[x^m]$ be the $m$-th moment of distribution $\xi(x)$. @gao2014new showed that the D-optimal design is symmetric in this case (i.e. $\eta_1=\eta_3=0$), and is given by 

\begin{equation*} 
\xi_D^* = \begin{cases} 
	\begin{bmatrix}
	-1			&	+1\\
    \frac{1}{2}	&	\frac{1}{2}
	\end{bmatrix},	 	& \mbox{for $t\in [0,\frac{2}{3})$}, \\
	\begin{bmatrix}
	-1			 &	0		&+1\\
    \frac{1}{3t} &\frac{3t-2}{3t}			&\frac{1}{3t}
	\end{bmatrix},				 					& \mbox{for $t\in [\frac{2}{3},1)$}.\\
\end{cases}
\end{equation*}

We now study the A-optimal design. By taking the advantage of the symmetric result in @yin2018optimal, we have
\[
	\boldsymbol{B}(\xi)=
    \begin{pmatrix}
    1			&0		&\sqrt{t}\eta_2\\
    0			&\eta_2	&0	\\
    \sqrt{t}\eta_2 &	0	&	\eta_4
    \end{pmatrix},~\text{and }
    \boldsymbol{B}^{-1}(\xi)=\begin{pmatrix} 
    	\frac{\eta_4}{\eta_4-t\eta_2}&0			&\frac{\sqrt{t}\eta_2}{t\eta_2^2-\eta_4}\\
        0		&\frac{1}{\eta_2}		&0\\
       \frac{\sqrt{t}\eta_2}{t\eta_2^2-\eta_4}&0&\frac{1}{\eta_4-t\eta_2^2} 
    \end{pmatrix}.
\]
From @dette1997theory, on $S=[-1,+1]$, the even moments of any distributions must satisfy $0\leq \eta_2^2 \leq \eta_4 \leq \eta_2 \leq 1$. Then our loss function can be expressed as 
\[
  \phi_A(\xi)=\operatorname{tr}(\boldsymbol{C}^\top \boldsymbol{B}^{-1}\boldsymbol{C})= \frac{1}{\eta_2}+\frac{1}{\eta_4-t\eta_2^2}.
\]
The optimal design problem can be written as 
\begin{equation*}
		\begin{aligned}
			& \underset{\eta_2,\eta_4}{\text{min}}
			& & \frac{1}{\eta_2}+\frac{1}{\eta_4-t\eta_2^2} \\
			& \text{s.t.}
			& & 0\leq \eta_2^2 \leq \eta_4 \leq \eta_2 \leq 1.
		\end{aligned}
	\end{equation*}
	In order to minimize the loss function, we first fix $\eta_2$. Then it is trivial to see we should make $\eta_4$ as big as possible, which leads to $\eta_4=\eta_2$, its ceiling. Now the question becomes 
	\begin{equation*}
		\begin{aligned}
			& \underset{\eta_2}{\text{min}}
			& & \frac{1}{\eta_2}+\frac{1}{\eta_2-t\eta_2^2} \\
			& \text{s.t.}
			& & 0\leq  \eta_2 \leq 1.
		\end{aligned}
	\end{equation*}
It yields to $\eta_2=\frac{2-\sqrt{2}}{t}$ or $\eta_2=\frac{2+\sqrt{2}}{t}$ where $t\ne 0$. Note $t\in[0,+1)$ and $\eta_2\in[0,+1]$, so the latter solution should be excluded as it is not within the feasible region. Consequently, 
\begin{equation*} 
\eta_2 = \begin{cases} 
	\min{(\frac{2-\sqrt{2}}{t},1)},	 	& \mbox{$t \in(0,1)$}, \\
	1,				 					& \mbox{$t = 0$}.\\
\end{cases}
\end{equation*}
By the symmetric property, the A-optimal design is 
\[	\xi_A^*=
	\begin{bmatrix}
    -1					&	0			&	+1\\
    \frac{\eta_2}{2}	&	1-\eta_2	&	\frac{\eta_2}{2}
    \end{bmatrix}.
\]
Here we have three cases:\\
\textbf{Case 1}
When $t=0$, 
\[	\xi_A^*=
	\begin{bmatrix}
    -1				&	+1\\
    \frac{1}{2}		&	\frac{1}{2}
    \end{bmatrix}.
\]
\textbf{Case 2}
When $t\in(0,2-\sqrt{2})$,
\[	\xi_A^*=
	\begin{bmatrix}
    -1				&	+1\\
    \frac{1}{2}		&	\frac{1}{2}
    \end{bmatrix}.
\]
\textbf{Case 3}
When $t\in[2-\sqrt{2},1)$,
\[	\xi_A^*=
	\begin{bmatrix}
    -1					&	0			&	+1\\
    \frac{2-\sqrt{2}}{2t}	&	1-\frac{2-\sqrt{2}}{t}	&	\frac{2-\sqrt{2}}{2t}
    \end{bmatrix}.
\]
In summary, the A-optimal design for \@ref(eq:model-poly) when $q=2$ is
\begin{equation*} 
\xi_A^* = \begin{cases} 
	\begin{bmatrix}
	-1			&	+1\\
    \frac{1}{2}	&	\frac{1}{2}
		\end{bmatrix},	 	& \mbox{for $t\in[0, \leq 2-\sqrt{2})$}, \\
	\begin{bmatrix}
	-1			&	0		&+1\\
    \frac{2-\sqrt{2}}{2t}	&\frac{2t+\sqrt{2}-2}{t}			&\frac{2-\sqrt{2}}{2t}
\end{bmatrix},				 					& \mbox{for $t\in[2-\sqrt{2},1)$}.\\
\end{cases}
\end{equation*}

From the results above, it is clear to observe that A- and D-optimal designs have either $2$ or $3$ support points which is consistent with Theorem \@ref(thm:support). 
\hfill\(\Box\)
:::

To show the result in Theorem \@ref(thm:dispersion), we plot $d_A(x,\xi^*,t)$ and $d_D(x,\xi^*,t)$ in Figure [TOFILL] using the optimal designs in Example \@ref(exm:poly2). It is clear that $d_A(x,\xi^*,t) \leq 0$ and $d_D(x,\xi^*,t)\leq 0$ for all $x\in S$, which is consistent with Theorem \ref{theorem:dispersion}. Also $d_A(x,\xi_A^*,t)=0$ at the support points of $\xi_A^*$ and $d_D(x,\xi_D^*,t)=0$ at the support points of $\xi_D^*$. As $t$ increases, the origin becomes a support point which changes the number  of support points from 2 to 3. This is again consistent with Theorem \@ref(thm:support\).

When the design space $S$ is asymmetric, say $S=[-1,b]$, $b\in(-1,1)$, the number of support points under A- and D-optimality are either $q$ or $q+1$ for all $q\in \mathbb{Z^+}$. When $t=0$, $n_D=q$ for all $q\in\mathbb{Z^+}$. The derivation is similar to that of Theorem \@ref(thm:support) and is omitted. We will show some computational results in Chapter \@ref(chapter:applications).

Fractional polynomial model (FPM) is given by 
\begin{equation}
 y=\theta_1x^{q_1}+\theta_2x^{q_2}+\cdots+\theta_px^{q_p}+\epsilon,~q_i\in\mathbb{Q},~\forall i,
 (\#eq:model-frac-poly)
\end{equation}
which provides more flexible parameterization with wide range of applications in many disciplines. For instance, @cui2009fractional used FPM for longitudinal epidemiological studies, and @royston1995regression applied this model to analyze medical data. This model has also been studied for optimal designs. For example, @torsney995minimally used FPM to study the effects of concentration $x$ to viscosity $y$ and this model is given by 
\begin{equation}
    y=\theta_1x+\theta_2x^{\frac{1}{2}}+\theta_3x^2+\epsilon,~x\in S=(0.0,0.2].
    (\#eq:ex-frac-poly)
\end{equation}
Let $z=x^{1/2}$, then \@ref(exm:frac_poly) becomes a polynomial model of order $4$ with the new design space $S'=(0.0,\sqrt{0.2}]$. Now, we can apply the result for the polynomial model \@ref(eq:model-poly) and conclude that $n_A$ and $n_D$ are at most $4$ or $5$. Moreover, notice that there are only three parameters in model \eqref{ex:frac_poly}. In order to have an invertible matrix $\boldsymbol{G_2}(\xi,\boldsymbol{\theta}_o)$ which is a $3\times 3$ matrix, we need at least $3$ support points. Thus, the number of support point for both A- and D-optimal design is at least 3. In summary, the number of support points in A- and D-optimal designs are either $3$, $4$, or $5$. The purpose of this example is for illustrating that FPM can be transformed into a polynomial model so we can use the result for the number of support points in polynomial models.

Michaelis-Menton model is one of the best-known models proposed by Leonor Michaelis and Maud Menten [@mm1913kinetik] that studies the enzyme reactions between the enzyme and the substrate concentration. This model is given by
\begin{equation}
y_i=\frac{\theta_1x_i}{\theta_2+x_i}+\epsilon_i.~ i=1,2,\dots,n,~ x_i \in(0,                                                                k_o],~\theta_1,\theta_2\geq 0.
(\#eq:model-mm)
\end{equation}
Here, $\boldsymbol{\theta}=(\theta_1,\theta_2)^\top$. Define $z=\frac{1}{\theta_2+x}$. Then $\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{x}{\theta_2+x},\frac{-\theta_1x}{(\theta_2+x)^2})^T= (1-z\theta_2, -z\theta_1 +\theta_1 \theta_2z^2)^\top$, and 
\begin{equation}
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1						& \sqrt{t} (1-z\theta_2)		&	\sqrt{t}(-\theta_1 z(1-z\theta_2))\\
\sqrt{t} (1-z\theta_2)	&(1-z\theta_2)^2			&	-z\theta_1(1-z\theta)^2\\
\sqrt{t}(-\theta_1 z(1-z\theta_2))&-z\theta_1(1-z\theta)^2&(z\theta_1(1-z\theta_2))^2
\end{pmatrix}.
\end{equation}

We can clearly see that, after the transformation, $z\in S'=[\frac{1}{\theta_1+k_o},\frac{1}{\theta_1}]$, and since $\boldsymbol{f}(x,\boldsymbol{\theta_o})$ and $\boldsymbol{M}(x,\boldsymbol{\theta},t)$ are polynomial functions of $z$, $\phi_D$ and $\phi_A$ can be described as polynomial functions with highest degrees $4$. Now, similar to the discussion for model \@ref(eq:model-poly), we can find the results for $n_A$ and $n_D$ for model \@ref(eq:model-mm} and they are presented in the following Lemma.

::: {.lemma #mm-support}
For Michaelis-Menton model in \@ref(eq:model-mm), the number of support points for D-optimal design and A-optimal design are either $2$ or $3$ for $t\in[0,1)$. Moreover, there are 2 support points under D-optimality when $t=0$.  
:::

The proof of Lemma \@ref(lem:mm-support) is similar to that of Theorem \ref{theorem:support} and is omitted. It is easy to observe that $d_D(0,\xi_D^*,0)<0$, so the boundary point, 0, is not a support point under D-optimality which leads to the conclusion $n_D=2$. Moreover, $n_c\le 3$ for $t\in[0,1)$.

Peleg model is a statistical model used to investigate the relationship of water absorption for various kinds of food. This model is given by
\begin{equation} 
y_i=y_o+\frac{x_i}{\theta_1+\theta_2 x_i}+\epsilon_i,\quad i=1,2,\cdots,n,
(\#eq:model-peleg)
\end{equation}
where $y_o$ represents the initial moisture of the food, $y_i$ is the current level of moisture at current time $x_i$, $\theta_1$ is the Peleg's moisture rate constant, and $\theta_2$ is the asymptotic moisture as time increases. Note that parameters $\theta_1$ and $\theta_2$ must be positive. Optimal experimental designs have been studied for this model using OLSE, for example, @paquet2015optimal. For \@ref(eq:model-peleg), $\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{-x}{(\theta_1+\theta_2x_i)^2},\frac{-x^2}{(\theta_1+\theta_2x)^2})^T$. We let $z=\frac{1}{\theta_1+\theta_2x}$. Then for $S=[0,d]$, the new design space after the transformation is $S'=[\frac{1}{\theta_1+\theta_2d},\frac{1}{\theta_1}]$. Now, $\boldsymbol{f}(x,\boldsymbol{\theta})=(\frac{-z(1-\theta_1z)}{\theta_2},\frac{-(1-\theta_1z)^2}{\theta_2^2})^T$. Since $\theta_1$ and $\theta_2$ are parameters, $\boldsymbol{f}$ depends on $z$ only. Hence, $\boldsymbol{M}(x,\boldsymbol{\theta},t)$ becomes a polynomial function of $z$ of degree $4$. Therefore, the number of support points, $n_A$ and $n_D$ are also either 2 or 3 for the Peleg model. In addition, $n_c$ is either 1, 2 or 3 for $t\in[0,1)$.


Next we consider trigonometric models without intercept. If the intercept term is included, it has been proven that optimal designs under SLSE and OLSE are the same. The $k$th order trigonometric model is given by 
\begin{equation} 
	y_i=\sum_{j=1}^{k}[cos(jx_i)\theta_{1j}+sin(jx_i)\theta_{2j}]+\epsilon_i,~i=1,2,\cdots,n,
	(\#eq:model-trig)
\end{equation}
where $x_i\in S_b=[-b\pi,b\pi],~0<b\leq 1$. Let $\boldsymbol{\theta}=(\theta_{11},\theta_{12},\cdots,\theta_{1k},\theta_{21},\cdots,\theta_{2k})^T$. From \@ref(eq:M-matrix), we get
\begin{equation} 
\boldsymbol{M}(x,\boldsymbol{\theta_o},t)=
\begin{pmatrix}
1		&\sqrt{t}cos(x)	&	\dots		&\sqrt{t}sin(kx)\\
\sqrt{t}cos(x)	&cos^2(x)	&\dots	&cos(x)sin(kx)\\
\vdots	&\vdots	&	\ddots 	&\vdots\\
\sqrt{t}sin(kx)	&cos(x)sin(kx)&\dots	&sin^2(kx)
\end{pmatrix}_{(2k+1)\times(2k+1)}.
(\#eq:trig-M-matrix)
\end{equation}
We consider two cases of design space, (i) $b=1$, full circle and (ii) $0<b<1$, the partial circle.

For case (i), the following Lemma is helpful for finding the number of support points for \@ref(eq:model-trig).

::: {.lemma #tring-property}
For trigonometric functions, $j, u =1,2,\cdots,k,~u\ne j$, we have 

1. $\int_{-\pi}^{\pi}cos(jx)dx=\int_{-\pi}^{\pi}sin(jx)dx=0$,
2. $\int_{-\pi}^{\pi}cos^2(jx)dx=\int_{-\pi}^{\pi}sin^2(jx)dx=\pi$,
3. $\int_{-\pi}^{\pi}cos(jx)cos(ux)dx=\int_{-\pi}^{\pi}sin(jx)sin(ux)dx=0$,
4. $\int_{-\pi}^{\pi}sin(jx)cos(ux)dx=\int_{-\pi}^{\pi}cos(jx)sin(ux)dx=0$. This also holds for $u=j$.
:::

::: {.theorem}
For model \@ref(eq:model-trig) with design space $S=[-\pi,\pi]$, the uniform distribution on $S$ is both A- and D-optimal designs. 
:::

::: {.proof}
For the uniform distribution $\xi^*$, we have $\boldsymbol{B}(\xi^*,\boldsymbol{\theta_o},t)=1\oplus \frac{1}{2} \boldsymbol{I_{2k}}$ by Lemma \@ref(lem:trig-property). From \@ref(eq:trig-M-matrix), we obtain
\begin{equation*}
\begin{aligned}
&tr(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))-(q+1)\\	&=1+2cos^2(x)+2cos^2(2x)+\dots+2sin^2(kx)-(2k+1)\\
&=1+2[cos^2(x)+sin^2(x)]+\dots+2[cos^2(kx)+sin^2(kx)]-(2k+1)\\
		&=1+2k-(2k+1)\\
        &=0,\quad \text{for all } x\in S.
\end{aligned}
\end{equation*}
This implies that $d_D(x,\xi^*,t)=0$ for all $x\in S$. By Theorem \@ref(thm:dispersion), $\xi^*$ is a D-optimal design.

For A-optimality, it is easy to get 
\begin{equation*}
\begin{aligned}
&tr(\boldsymbol{M}(x,\boldsymbol{\theta_o},t)\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t)\boldsymbol{C}^T\boldsymbol{C}\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))-\operatorname{tr}(\boldsymbol{C}\boldsymbol{B}^{-1}(\xi^*,\boldsymbol{\theta_o},t))\\&=4cos^2(x)+4cos^2(2x)+\dots+4sin^2(kx)-4k\\
&=4[cos^2(x)+sin^2(x)]+\dots+4[cos^2(kx)+sin^2(kx)]-4k\\
&=4k-4k\\
&=0,\quad \text{for all } x\in S,
\end{aligned}
\end{equation*}
which gives $d_A(x,\xi^*,t)=0$ for all $x \in S$. Thus, by Theorem \@ref(thm:dispersion), $\xi^*$ is an A-optimal design.
:::

For case (ii), $0<b<1$, the partial circle $S=[-b\pi,+b\pi]$, let $z=cos(x)$. Now, instead of using $x$ directly, we study the number of support point of $x$ through $z$ in $S'=[cos(b\pi),1]$. Note that cosine is an even function, so each point of $z\in S'$ corresponds to two symmetric points around $0$, $\pm x\in S$. @gao2017d discussed that all the elements in $\boldsymbol{M}(x,\boldsymbol{\theta_o},t)$ in \@ref(eq:trig-M-matrix) can be written as polynomial functions of $z$. Hence, functions $d_A(x,\xi,t)$, $d_D(x,\xi,t)$ and $d_c(x,\xi,t)$ are all polynomial functions of $z$ with degree $2k$. Using the similar arguments for the polynomial models, A-, D- and c-optimal designs, in terms of $z\in S'$, cannot exceed $k+1$ support points.

## Scale invariance property of D-optimal design

D-optimality is one of the most used design criteria due to its many advantages. One good property of this criterion is that it is invariant under some scaling of the independent variables using OLSE [@berger2009introduction]. Recently there are some discussions about the invariance properties including scale invariance and shift invariance for the optimal designs under the SLSE. Examples can be found in @gao2014new and @yin2018optimal. In this section, we focus on finding a new property of scale invariance of D-optimal designs under the SLSE for nonlinear regression models.

For linear regression models, D-optimal designs are often scale invariant. On the other hand, if the model is nonlinear, the scale invariance property is no longer available. The optimal designs for nonlinear models are called locally optimal, since they depend on the true parameter vector $\boldsymbol{\theta_o}$. Thus, D-optimal designs have to be constructed for each $\boldsymbol{\theta_o}$ and for each $S$. @wong2019cvx proposed a generalized scale invariance (GSI) concept for studying scale invariance property of D-optimal designs for nonlinear models and generalized linear models. The GSI property is also useful for studying D-optimal designs under the SLSE such that the designs may be constructed on a scaled design space $S^V$ instead of the original design space $S$.

Denote the D-optimal design for a given model, with true parameter vector $\boldsymbol{\theta}_o$ on a design space $S$ by $\xi_D^*(S,\boldsymbol{\theta}_o)$. Also, denote the scaled design space from the original design space $S$ by $S^{V}=\{\boldsymbol{V}\boldsymbol{x}|\boldsymbol{x}\in S\}$.

::: {.definition #scale-matrix}
The matrix $\boldsymbol{V}$ is called scale matrix which is a diagonal matrix defined as $\boldsymbol{V}=diag(v_1,v_2,\cdots,v_p)$, where all $v_i$ are positive.
:::

::: {.definition #scale-invariant}
Transforming $\boldsymbol{x}$ to $\boldsymbol{V} \boldsymbol{x}$ is called scale transformation.
:::

::: {.definition #GSI}
$\xi_D^*(S,\boldsymbol{\theta}_o)$ is said to be scale invariant for a model if there exists a parameter vector $\tilde{\boldsymbol{\theta_o}}$ such that the D-optimal design $\xi_D^*(S^V,\tilde{\boldsymbol{\theta_o}})$ can be obtained from $\xi_D^*(S,\boldsymbol{\theta_o})$ using some scale transformations.
:::

This property of the design, $\xi_D^*(S,\boldsymbol{\theta_o})$, is defined as a generalized scale invariance. Note that $\tilde{\boldsymbol{\theta_o}}$ and $\boldsymbol{\theta_o}$ are closed related through the elements $v_1,\cdots,v_p$ in the scale matrix $\boldsymbol{V}$. When the model is linear, GSI of $\xi_D^*(S,\boldsymbol{\theta_o})$ becomes the traditional scale invariance since it does not depend on $\boldsymbol{\theta_o}$. For many nonlinear regression models, the GSI property holds for D-optimal designs under both OLSE and SLSE. The following lemma provides a condition to check for this property.

::: {.lemma #GSI}
For a given model and a scale matrix $\boldsymbol{V}$, if there exists a parameter vector $\tilde{\boldsymbol{\theta}_o}$ and an invertible diagonal matrix $\boldsymbol{K}$ which does not depend on $\boldsymbol{x}$, such that $\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta}_o)$ for all $\boldsymbol{x}\in S$, then the design $\xi_D^*(S,\boldsymbol{\theta}_o)$ has the generalized scale invariance property.
\end{lemma}
:::

::: {.proof}
For design space $S$ and parameter vector $\boldsymbol{\theta_o}$, $\xi_D^*(S,\boldsymbol{\theta_o})$ minimizes $\phi_D^*(\xi,\boldsymbol{\theta_o})=\det(\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta_o}))$ where $\boldsymbol{B}(\xi,\boldsymbol{\theta}_o)=\mathbb{E}[\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)]$ and $\boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)$ is given by \eqref{M matrix}, and the expectation is taken with respect to $\xi(\boldsymbol{x})$ on $S$.


For design space $S^V=\{\boldsymbol{V} \boldsymbol{x}|\boldsymbol{x}\in S\}$ and parameter vector $\tilde{\boldsymbol{\theta_o}}$, we minimize the following function to find $\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})$, $\phi_D^*(\xi,\tilde{\boldsymbol{\theta}_o})=\det(\boldsymbol{B}^{-1}(\xi,\tilde{\boldsymbol{\theta}_o}))$, where $\boldsymbol{B}(\xi,\tilde{\boldsymbol{\theta}_o})=\mathbb{E}[\boldsymbol{M}(\boldsymbol{z},\tilde{\boldsymbol{\theta}_o})]$, and the expectation is taken with respect to $\xi(\boldsymbol{z})$ on $S^V$. Each $\boldsymbol{z}\in S^V$ can be written as $\boldsymbol{V}\boldsymbol{x}$ followed by definition, where $\boldsymbol{x}\in S$. Thus, we have $\boldsymbol{M}(\boldsymbol{z},\tilde{\boldsymbol{\theta}_o})=\boldsymbol{M}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})$.

By the assumption of Lemma \@ref(lem:GSI) and \@ref(eq:M-matrix), we get \[
\boldsymbol{M}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})=(1 \oplus \boldsymbol{K}) \boldsymbol{M}(\boldsymbol{x},\boldsymbol{\theta}_o)(1 \oplus \boldsymbol{K}),~for~all~\boldsymbol{x}\in S.
\]
Therefore, $\boldsymbol{B}(\xi,\tilde{\boldsymbol{\theta}_o})= (1 \oplus \boldsymbol{K}) \boldsymbol{B}(\xi,\boldsymbol{\theta}_o)(1 \oplus \boldsymbol{K})$ and $\phi_D(\xi,\tilde{\boldsymbol{\theta}_o})=\frac{\phi_D(\xi,\boldsymbol{\theta}_o)}{\det^2(\boldsymbol{K})}$. This implies that we can minimize $\phi_D(\xi,\boldsymbol{\theta}_o)$ to obtain $\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})$, where $\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})$ is the scale transformation from $\xi_D^*(S,\boldsymbol{\theta}_o)$.
:::

::: {.example #peleg}
Consider Peleg regression model in \@ref(eq:model-peleg). In this model, since $p=1$, the scale matrix $V=v_1>0$ is a scalar and $\boldsymbol{\theta}_o=(a,b)^\top$. Now, let $\tilde{\boldsymbol{\theta}_o}=(a,\frac{b}{v_1})^\top$ and we can obtain

\begin{equation*}
\begin{aligned}
\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta}_o})	
		&= \boldsymbol{f}(v_1x,\tilde{\boldsymbol{\theta}_o})\\
		&= \begin{pmatrix}\frac{-v_1x}{(a+\frac{b}{v_1}v_1x)^2}, & \frac{-(v_1x)^2}{(a+\frac{b}{v_1}v_1x)^2}\end{pmatrix}^\top\\
        &=\begin{pmatrix} v_1\frac{-x}{(a+bx)^2}, & v_1^2\frac{-x^2}{(a+bx)^2}\end{pmatrix}^\top\\
        &= \begin{pmatrix} v_1	&0\\0&v_1^2\end{pmatrix}\begin{pmatrix} \frac{-x}{(a+bx)^2}, & \frac{-(v_1x)^2}{(a+bx)^2}\end{pmatrix}^\top\\
        &=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta_o}).
\end{aligned}
\end{equation*}
Hence, Peleg model has GSI property based on Lemma \@ref(lem:GSI) by choosing $\boldsymbol{K}=diag(v_1,v_1^2)$ and $\tilde{\boldsymbol{\theta_o}}=(a,\frac{b}{v_1})^T$.	\hfill\(\Box\)
:::

It is worth noting that when matrix $\boldsymbol{B}(\xi,\boldsymbol{\theta}_o,t)$ is ill-conditioned (i.e. the condition number of the matrix is very large), numerical algorithm computing $\boldsymbol{B}^{-1}(\xi,\boldsymbol{\theta}_o,t)$ may fail as the numerical inverse is inaccurate and imprecise. In these situations, the GSI property may be helpful. Here we provide one example to demonstrate the usefulness of the GSI property.

::: {.example}
Piecewise polynomial regression using knots is frequently used and has various applications. See @dette2008optimal and the references therein. They investigated optimal designs under OLSE for piecewise polynomial regression model with unknown knots, and obtained results for the number of support points under D-optimality and various other properties about designs. Here we consider one model they used, a cubic spline regression model with unknown knots, which is given by 
\begin{equation}
y=\theta_1+\theta_2x+\theta_3x^2+\theta_4x^3+\theta_5(x-\lambda)_{+}^3+\epsilon,~x\in [0,b]
(\#eq:spline-regression)
\end{equation}
where $(x-\lambda)_{+} = \max(0,(x-\lambda))$. Model \eqref{model:spline_regression} is nonlinear with parameter vector $\boldsymbol{\theta}_o=(\theta_1,\cdots,\theta_5,\lambda)^T$, $\boldsymbol{f}(x,\boldsymbol{\theta}_o)=(1,x,x^2,x^3,(x-\lambda)_+^3,-3\theta_5(x-\lambda)_+^2)^T$. We now start to illustrate GSI property for this model. Consider a scale matrix $\boldsymbol{V}=v_1>0$ and let $\tilde{\boldsymbol{\theta}_o}=(\theta_1,\cdots,\theta_5,v_1\lambda)^T$. Then we have, for all $x\in[0,b]$,

\begin{equation*}
\begin{aligned}
\boldsymbol{f}(\boldsymbol{V}\boldsymbol{x},\tilde{\boldsymbol{\theta_o}})	
		&= \boldsymbol{f}(v_1x,\tilde{\boldsymbol{\theta_o}})\\
        &= (1, v_1x,  (v_1x)^2,  (v_1x)^3,  (v_1x-v_1\lambda)_+^3,  -3\theta_5(v_1x-v_1\lambda)_+^2)^T\\
        &= (1,  v_1,x,  (v_1x)^2,  (v_1x)^3,  v_1^3(x-\lambda)_+^3,  -3\theta_5v_1^2(x-\lambda)_+^2 )^T\\
        &= diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2) (1, x,  x^2, x^3, (x-\lambda)_+^3, -3\theta_5(x-\lambda)_+^2)^T\\
        &=\boldsymbol{K}\boldsymbol{f}(\boldsymbol{x},\boldsymbol{\theta_o}),\quad \text{with } \boldsymbol{K}=diag(1,v_1,v_1^2,v_1^3,v_1^3,v_1^2).
\end{aligned}
\end{equation*}
Hence, by Lemma \@ref(lem:GSI), the D-optimal design under SLSE has the GSI property. Moreover, the model is linear in $\boldsymbol{\theta}_o'=(\theta_1,\theta_2,\cdots,\theta_5)^\top$, so the D-optimal designs under both OLSE and SLSE do not depend on $\boldsymbol{\theta}_o'$ @yin2018optimal. Thus, for $S=[0,b]$ and $S^{V}=[0,v_1b]$, the D-optimal designs $\xi_D^*(S,\boldsymbol{\theta}_o)$ and $\xi_D^*(S^V,\tilde{\boldsymbol{\theta}_o})$ can be obtained from each other by using the corresponding $\boldsymbol{\theta}_o$ and $\tilde{\boldsymbol{\theta}}$. This property can help us dramatically for finding the D-optimal designs when matrix $\boldsymbol{B}(\xi,\boldsymbol{\theta}_o,t)$ is ill-conditioned. Numerical results of D-optimal designs are presented in Example \ref{example:spline} after we discuss about numerical algorithms in Chapter \@ref(chapter:applications). \hfill\(\Box\)
:::

We also want to note that in model \@ref(eq:spline-regression), the intercept is included so the D-optimal designs under both OLSE and SLSE are the same [@gao2014new]. Moreover, the result may easily be extended for piecewise regression models with multiple unknown knots. 
